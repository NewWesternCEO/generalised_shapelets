\section{Experiments}
Our generalised shapelet transform, contrasted with the classical shapelet transform, has two extra degrees of freedom: the choice of interpolation scheme $\iota$, and the choice of discrepancy function $\pi^A_S$. In our experiments, we consider $\pi^A_S$ given by either of equations \eqref{eq:learnt-discrepancy} or \eqref{eq:logsignature-discrepancy} and in both cases take the discrepancy parameters $A$ to be diagonal, as this was empirically found empirically to perform best, and the truncation depth of the log-signature is always taken to be $3$. Meanwhile, we take $\iota$ to be piecewise linear interpolation, because efficient algorithms for computing the logsignature transform only exist for piecewise linear paths \cite{signatory}.

For old shapelets, the key hyperparameters to consider are the number of shapelets and the length the these shapelets correspond to. A full description of the hyperparameter selection process is given in Appendix \ref{}. In short we perform a small hyperparameter search for each dataset to optmise the number and length of the shapelets for the old method and use these same parameters in the generalised approach. The length hyperparameter has less meaning in the generalised approach as it is changed during the learning process, but is used in the shapelet initialisation scheme such that the initialised shapelts are the same in all cases.

\subsection{The UEA (Multivariate) Time-Series Archive}
We begin by comparing the classification performance of the old method of shapelets to our new `generalised' approach for which we consider three learnt metrics: a standard L2-metric, an learnt metric and a logsig-3-diagonal. We evaluate the methods on a subset of the UEA time-series archive \cite{bagnall2018uea}. This contains a wide range of multivariate time-series classification problems from various fields with significant differences in time-series length, number of classes, and amount of training data. The full collection contains 30 datasets, however due to algorithm run-time constraints we have considered a subset of these ensuring they still contain significant variation. The statistics of these datasets are given in Table \ref{} of the Appendix.

The results are given in Table \ref{tab:uea_comparison_results}. We see significantly improved classification performance ($> 1$ standard deviation) on $6$ of the $9$ datasets tested with $1$ draw and $2$ losses. We note that the loss on `PenDigits' was extemely close, and the other loss was on `BasicMotions' which has a very small total number samples ($80$) making overfit more likely on the generalised methods (without proper hyperparameter tuning) due to the additional parameters in the model. In contrast, there are many cases where the new method won with significantly higher accuracies than the old method. We also note that the method is sensitive to choice of hyperparameters, and all hyperparameters were chosen for the old method, if hyperparameters were chosen separately for each discrepancy, this would likely improve the results for the learnt discrepancies further.
\begin{table}[ht]
    \centering
    \caption{Classification performance on the UEA datasets from the old shapelet method and the generalised method with a diagonal L2 and diagonal logsig metric with depth 3. The values indicate the classification accuracy on the test set plus or minus one standard deviation. The wins are computed as the number of times each algorithm was within 1 standard deviation from the top score for each dataset.}
    \label{tab:uea_comparison_results}
    \input{results/data/uea_comparison_new.tex}
\end{table}


\subsubsection{Algorithm Interpretability}
Here we explore the effect of the inclusion of the regularisation term from Equation \cite{eq:interpretable_reg} on shapelet interpretability. Recall that the term was chosen to ensure the resulting shapelets are `close', in the sense of the chosen discrepancy, to some subsample of the training data. To examine this we consider the PenDigits dataset where participants were asked to write down a number from 0-9 and the goal is to classify the intended digit. In Figure \ref{fig:pendigits} we plot, for each digit (0 to 9 in order from left to right), the learnt shapelet that corresponds to the largest coefficient from the logistic-regression for that class. For the old method \ref{fig:old_shapelets}, it is in general not clear what aspect of the digit the shapelet is representing. In some cases the closest digit to the shapelet is not even of the class for which it is thought to best discriminate. Furthermore, the 0 and 1 classes along with the 5 and 8 classes share their `top' shapelet. In contrast, it is abundantly clear what aspect of the digit is being captured by the top shapelets produced from our generalised method \ref{fig:new_shapelets}. Some particular points of interest include shape of the 5 and 6 shapelet being very similar, but the distinction being found in the shapelets chirality (along with its thickness) as the bottom half of a 5 is written clockwise and the 6 anticlockwise. The shapelet for the 7 digit is interesting in its simplicity with it not being immediately clear what makes it so discriminative. By considering those digits with strokes in the top left hand region, we can see that the 8 and 9 have different chiralities to the 7, and the strokes for 1, 2 and 3 are generally written curving upwards to being with wheras 7 is usually flat. A similar such case can be made to explain why the shapelet for the 2 is found to be highly discriminative.

We chose PenDigits here because it is easy to understand visually what the shapelet represents. In general this is not the case, but provided we can make sense of isolated subintervals of the time-series, then the generated results will be interpretable in this context.
\begin{figure}[ht]
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/pendigits/old_shapelets_noborder.png}
        \caption{Old shapelets.}
        \label{fig:old_shapelets}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/pendigits/new_shapelets_noborder.png}
        \caption{New shapelets.}
        \label{fig:new_shapelets}
    \end{subfigure}
    \caption{The learnt shapelet that corresponds to the largest coefficient in the logistic regression layer for the digits 0 to 9 in order from left to right (blue) and the `closest' path (the path with smallest discrepancy) from the training data (orange dashed).}
    \label{fig:pendigits}
\end{figure}


\subsubsection{Missing Data and Length Ablation}
We now demonstrate both the ability of the proposed framework to handle partially observed data, as well as understand the effectiveness of having learnable shapelet lengths. We consider three datasets from the UEA archive and the two learnt discrepancy functions we have been using in this section (L2-diagonal and logsig-3). For each dataset and discrepancy function we run six experiments where we drop $10\%$, $30\%$, and $50\%$ of the data in the situation where we allow length to be learnt and when we keep the length fixed. The results are given in Table \ref{tab:uea_noise}. Firstly, we note that the model is extremely robust to missing data as the performance is in general maintained, or close to maintained, even as we drop $50\%$ of the data-points. Allowing the length to be learnt does not appear to improve the performance of the model in terms of accuracy, however we postulate that allowing for learnt lengths can take the place of the hyperparamter search over the length.
\begin{table}[ht]
    \caption{}
    \label{tab:uea_noise}
    \centering
    \input{results/data/uea_missing_and_length.tex}
\end{table}


\subsection{Speech Commands}
Finally we examine the performance on the speech commands dataset \ref{warden2018speech} which includes a selection of one-second audio files with each representing a single spoken word. The aim is to build a model to detect the word that has been spoken. We chose this dataset as it is significantly larger than any in the UEA archive so as to demonstrate that the method is not restricted to these (relatively) small datasets. We do reiterate that the computational cost of shapelet methods is high in comparison to other more traditional deep-learning approaches and so can make datasets such as this prohibitive, in particular, it is why we have left the logsig-3 discrepancy out of our analysis in this instance as it takes significantly longer to train compared with the L2 methods.
\begin{table}[ht]
    \caption{Classification accuracy for old shapelets and new shapelets on the Speech Commands dataset.}
    \label{tab:speech_commands}
    \centering
    \input{results/data/speech_commands_placeholder.tex}
\end{table}

\subsubsection{Interpretability of Speech Commands}
Pray for me.

