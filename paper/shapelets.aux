\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ye2009firstshapelet}
\citation{grabocka2014learningshapelet}
\citation{hou2016efficient}
\citation{bagnall2016bakeoff}
\citation{hills2014classification}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Classical shapelet transform}{1}{subsection.1.1}}
\newlabel{section:classical-introduction}{{1.1}{1}{Classical shapelet transform}{subsection.1.1}{}}
\newlabel{eq:f-n}{{1}{1}{Classical shapelet transform}{equation.1.1}{}}
\citation{ye2009firstshapelet}
\citation{grabocka2014learningshapelet}
\citation{wang2019interp}
\citation{ye2009firstshapelet}
\citation{mueen2011logical}
\citation{grabocka2015scalable}
\citation{grabocka2016speedshapelet}
\citation{rak2013fast}
\citation{wistuba2015ultrafast}
\citation{grabocka2014learningshapelet}
\citation{grabocka2014learningshapelet}
\citation{tensorflow}
\citation{pytorch}
\citation{jax}
\citation{wang2019interp}
\citation{wang2019interp}
\citation{grabocka2016dtwshapelet}
\citation{bagnall2016bakeoff}
\citation{bostrom2015shapelet}
\newlabel{eq:classical-shapelets}{{2}{2}{Classical shapelet transform}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Limitations}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Prior work}{2}{section.2}}
\citation{interpolation-prediction}
\citation{li2016scalable}
\citation{futoma2017mgp}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Continuous-time objects}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Generalised discrepancy}{3}{subsection.3.2}}
\newlabel{eq:new-sigma}{{3}{3}{Generalised discrepancy}{equation.3.3}{}}
\citation{logsig-rnn}
\citation{mfc}
\citation{logsig-rnn}
\citation{levy-lyons}
\citation{kidger2019deep}
\citation{signatory}
\citation{witt}
\citation{mfc}
\citation{hills2014classification}
\citation{wang2019interp}
\newlabel{eq:learnt-discrepancy}{{4}{4}{Generalised discrepancy}{equation.3.4}{}}
\newlabel{eq:logsignature-discrepancy}{{5}{4}{Generalised discrepancy}{equation.3.5}{}}
\newlabel{eq:mfc-discrepancy}{{6}{4}{Generalised discrepancy}{equation.3.6}{}}
\citation{signatory}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Interpretable regularisation}{5}{subsection.3.3}}
\newlabel{eq:interpretable_reg}{{7}{5}{Interpretable regularisation}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Minimisation objective and training procedure}{5}{subsection.3.4}}
\newlabel{eq:objective}{{8}{5}{Minimisation objective and training procedure}{equation.3.8}{}}
\newlabel{section:choice-of-f}{{3.4}{5}{Minimisation objective and training procedure}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{section:experiments}{{4}{5}{Experiments}{section.4}{}}
\citation{kingma2015}
\citation{bagnall2018uea}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test accuracy (mean $\pm $ std, computed over three runs) on UEA. A `win' is the number of times each algorithm was within 1 standard deviation of the top performer for each dataset.\relax }}{6}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:uea_comparison_results}{{1}{6}{Test accuracy (mean $\pm $ std, computed over three runs) on UEA. A `win' is the number of times each algorithm was within 1 standard deviation of the top performer for each dataset.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The UEA Time Series Archive}{6}{subsection.4.1}}
\citation{warden2018speech}
\newlabel{fig:old_shapelets}{{1a}{7}{Classical shapelet transform.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:old_shapelets}{{a}{7}{Classical shapelet transform.\relax }{figure.caption.3}{}}
\newlabel{fig:new_shapelets}{{1b}{7}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:new_shapelets}{{b}{7}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The most significant shapelet for each class (blue, solid), overlaid with the most similar training example (orange, dashed). Similarity is measured with respect to the (learnt) discrepancy function.\relax }}{7}{figure.caption.3}}
\newlabel{fig:pendigits}{{1}{7}{The most significant shapelet for each class (blue, solid), overlaid with the most similar training example (orange, dashed). Similarity is measured with respect to the (learnt) discrepancy function.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Missing Data and Length Ablation}{7}{subsection.4.2}}
\newlabel{sec:missing_and_length}{{4.2}{7}{Missing Data and Length Ablation}{subsection.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \relax }}{8}{table.caption.4}}
\newlabel{tab:uea_noise}{{2}{8}{\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Speech Commands}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\bibstyle{ieeetr}
\bibdata{references}
\bibcite{ye2009firstshapelet}{1}
\bibcite{grabocka2014learningshapelet}{2}
\bibcite{hou2016efficient}{3}
\bibcite{bagnall2016bakeoff}{4}
\bibcite{hills2014classification}{5}
\bibcite{wang2019interp}{6}
\bibcite{mueen2011logical}{7}
\bibcite{grabocka2015scalable}{8}
\bibcite{grabocka2016speedshapelet}{9}
\bibcite{rak2013fast}{10}
\bibcite{wistuba2015ultrafast}{11}
\bibcite{tensorflow}{12}
\newlabel{fig:old_speech_commands}{{2a}{9}{Classical shapelet transform.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:old_speech_commands}{{a}{9}{Classical shapelet transform.\relax }{figure.caption.5}{}}
\newlabel{fig:new_speech_commands}{{2b}{9}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:new_speech_commands}{{b}{9}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First 10 MFC coefficients for the shapelet (left), training set minimizer (middle), and the difference between them (right).\relax }}{9}{figure.caption.5}}
\newlabel{fig:speech_commands}{{2}{9}{First 10 MFC coefficients for the shapelet (left), training set minimizer (middle), and the difference between them (right).\relax }{figure.caption.5}{}}
\bibcite{pytorch}{13}
\bibcite{jax}{14}
\bibcite{grabocka2016dtwshapelet}{15}
\bibcite{bostrom2015shapelet}{16}
\bibcite{interpolation-prediction}{17}
\bibcite{li2016scalable}{18}
\bibcite{futoma2017mgp}{19}
\bibcite{logsig-rnn}{20}
\bibcite{mfc}{21}
\bibcite{levy-lyons}{22}
\bibcite{kidger2019deep}{23}
\bibcite{signatory}{24}
\bibcite{witt}{25}
\bibcite{kingma2015}{26}
\bibcite{bagnall2018uea}{27}
\bibcite{warden2018speech}{28}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental details}{11}{appendix.A}}
\newlabel{appendix:experimental}{{A}{11}{Experimental details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}General notes}{11}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}UEA}{11}{subsection.A.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Speech Commands}{11}{subsection.A.3}}
\citation{tensorflow}
\citation{pytorch}
\citation{jax}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Further speech commands interpretability.}{12}{subsection.A.4}}
\newlabel{apx:further_speech_commands_interpretability}{{A.4}{12}{Further speech commands interpretability}{subsection.A.4}{}}
\newlabel{fig:old_speech_commands_axisplot}{{3a}{13}{Classical shapelet transform.\relax }{figure.caption.9}{}}
\newlabel{sub@fig:old_speech_commands_axisplot}{{a}{13}{Classical shapelet transform.\relax }{figure.caption.9}{}}
\newlabel{fig:new_speech_commands_axisplot}{{3b}{13}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.9}{}}
\newlabel{sub@fig:new_speech_commands_axisplot}{{b}{13}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces All MFC coefficients for the shapelet (blue) and the training set minimizer (orange, dashed).\relax }}{13}{figure.caption.9}}
\newlabel{fig:speech_commands_axisplot}{{3}{13}{All MFC coefficients for the shapelet (blue) and the training set minimizer (orange, dashed).\relax }{figure.caption.9}{}}
