\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ye2009firstshapelet}
\citation{grabocka2014learningshapelet}
\citation{hou2016efficient}
\citation{bagnall2016bakeoff}
\citation{hills2014classification}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Classical shapelet transform}{1}{subsection.1.1}}
\newlabel{section:classical-introduction}{{1.1}{1}{Classical shapelet transform}{subsection.1.1}{}}
\newlabel{eq:f-n}{{1}{1}{Classical shapelet transform}{equation.1.1}{}}
\citation{ye2009firstshapelet}
\citation{grabocka2014learningshapelet}
\citation{wang2019interp}
\citation{ye2009firstshapelet}
\citation{mueen2011logical}
\citation{grabocka2015scalable}
\citation{grabocka2016speedshapelet}
\citation{rak2013fast}
\citation{wistuba2015ultrafast}
\citation{grabocka2014learningshapelet}
\citation{grabocka2014learningshapelet}
\citation{tensorflow}
\citation{pytorch}
\citation{jax}
\citation{wang2019interp}
\citation{wang2019interp}
\citation{grabocka2016dtwshapelet}
\citation{bagnall2016bakeoff}
\citation{bostrom2015shapelet}
\newlabel{eq:classical-shapelets}{{2}{2}{Classical shapelet transform}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Limitations}{2}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contributions}{2}{subsection.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Prior work}{2}{section.2}}
\citation{interpolation-prediction}
\citation{li2016scalable}
\citation{futoma2017mgp}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Continuous-time objects}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Generalised discrepancy}{3}{subsection.3.2}}
\newlabel{eq:new-sigma}{{3}{3}{Generalised discrepancy}{equation.3.3}{}}
\citation{logsig-rnn}
\citation{mfc}
\citation{logsig-rnn}
\citation{levy-lyons}
\citation{kidger2019deep}
\citation{signatory}
\citation{witt}
\citation{mfc}
\citation{hills2014classification}
\citation{wang2019interp}
\newlabel{eq:learnt-discrepancy}{{4}{4}{Generalised discrepancy}{equation.3.4}{}}
\newlabel{eq:logsignature-discrepancy}{{5}{4}{Generalised discrepancy}{equation.3.5}{}}
\newlabel{eq:mfc-discrepancy}{{6}{4}{Generalised discrepancy}{equation.3.6}{}}
\citation{signatory}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Interpretable regularisation}{5}{subsection.3.3}}
\newlabel{eq:interpretable_reg}{{7}{5}{Interpretable regularisation}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Minimisation objective and training procedure}{5}{subsection.3.4}}
\newlabel{eq:objective}{{8}{5}{Minimisation objective and training procedure}{equation.3.8}{}}
\newlabel{section:choice-of-f}{{3.4}{5}{Minimisation objective and training procedure}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{section:experiments}{{4}{5}{Experiments}{section.4}{}}
\citation{kingma2015}
\citation{bagnall2018uea}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test accuracy (mean $\pm $ std, computed over three runs) on UEA. A `win' is the number of times each algorithm was within 1 standard deviation of the top performer for each dataset.\relax }}{6}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:uea_comparison_results}{{1}{6}{Test accuracy (mean $\pm $ std, computed over three runs) on UEA. A `win' is the number of times each algorithm was within 1 standard deviation of the top performer for each dataset.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}The UEA Time Series Archive}{6}{subsection.4.1}}
\newlabel{subsec:uea_classification}{{4.1}{6}{The UEA Time Series Archive}{subsection.4.1}{}}
\newlabel{fig:old_shapelets}{{1a}{7}{Classical shapelet transform.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:old_shapelets}{{a}{7}{Classical shapelet transform.\relax }{figure.caption.3}{}}
\newlabel{fig:new_shapelets}{{1b}{7}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:new_shapelets}{{b}{7}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The most significant shapelet for each class (blue, solid), overlaid with the most similar training example (orange, dashed). Similarity is measured with respect to the (learnt) discrepancy function.\relax }}{7}{figure.caption.3}}
\newlabel{fig:pendigits}{{1}{7}{The most significant shapelet for each class (blue, solid), overlaid with the most similar training example (orange, dashed). Similarity is measured with respect to the (learnt) discrepancy function.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Missing Data and Length Ablation}{7}{subsection.4.2}}
\newlabel{subsec:uea_missing_and_length}{{4.2}{7}{Missing Data and Length Ablation}{subsection.4.2}{}}
\citation{warden2018speech}
\newlabel{tab:uea_noise}{{\caption@xref {tab:uea_noise}{ on input line 315}}{8}{Missing Data and Length Ablation}{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Test accuracy (mean $\pm $ std, computed over three runs) on 3 of the UEA datasets for different proportions of dropped data and the cases where learnt length is enabled and when length is fixed. A `win' is defined as the number of times each algorithm was within 1 standard deviation of the top performer for each dataset.\relax }}{8}{table.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Speech Commands}{8}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{8}{section.5}}
\bibstyle{ieeetr}
\bibdata{references}
\bibcite{ye2009firstshapelet}{1}
\bibcite{grabocka2014learningshapelet}{2}
\bibcite{hou2016efficient}{3}
\bibcite{bagnall2016bakeoff}{4}
\bibcite{hills2014classification}{5}
\bibcite{wang2019interp}{6}
\bibcite{mueen2011logical}{7}
\bibcite{grabocka2015scalable}{8}
\bibcite{grabocka2016speedshapelet}{9}
\newlabel{fig:old_speech_commands}{{2a}{9}{Classical shapelet transform.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:old_speech_commands}{{a}{9}{Classical shapelet transform.\relax }{figure.caption.5}{}}
\newlabel{fig:new_speech_commands}{{2b}{9}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:new_speech_commands}{{b}{9}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First 10 MFC coefficients for the shapelet (left), training set minimizer (middle), and the difference between them (right).\relax }}{9}{figure.caption.5}}
\newlabel{fig:speech_commands}{{2}{9}{First 10 MFC coefficients for the shapelet (left), training set minimizer (middle), and the difference between them (right).\relax }{figure.caption.5}{}}
\bibcite{rak2013fast}{10}
\bibcite{wistuba2015ultrafast}{11}
\bibcite{tensorflow}{12}
\bibcite{pytorch}{13}
\bibcite{jax}{14}
\bibcite{grabocka2016dtwshapelet}{15}
\bibcite{bostrom2015shapelet}{16}
\bibcite{interpolation-prediction}{17}
\bibcite{li2016scalable}{18}
\bibcite{futoma2017mgp}{19}
\bibcite{logsig-rnn}{20}
\bibcite{mfc}{21}
\bibcite{levy-lyons}{22}
\bibcite{kidger2019deep}{23}
\bibcite{signatory}{24}
\bibcite{witt}{25}
\bibcite{kingma2015}{26}
\bibcite{bagnall2018uea}{27}
\bibcite{warden2018speech}{28}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental details}{11}{appendix.A}}
\newlabel{appendix:experimental}{{A}{11}{Experimental details}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}General notes}{11}{subsection.A.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}UEA}{11}{subsection.A.2}}
\newlabel{apx:uea}{{A.2}{11}{UEA}{subsection.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \relax }}{11}{table.caption.9}}
\newlabel{tab:uea_hyperparams_old}{{3}{11}{\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \relax }}{11}{table.caption.10}}
\newlabel{tab:uea_hyperparams_l2}{{4}{11}{\relax }{table.caption.10}{}}
\newlabel{RF1}{12}
\newlabel{tab:uea_hyperparams_old_results}{{\caption@xref {tab:uea_hyperparams_old_results}{ on input line 434}}{12}{UEA}{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Accuracy on the validation set for the hyperparmeter runs performed to determine the hyperparameters used in Section \ref  {subsec:uea_classification}. The top column value represents the number of classes per shapelet for the run, with the lower values being the shapelet length proportion for that number of classes. The best run is given in bold. When multiple options achieved the highest score, the hyperparameters were chosen randomly from that top performing set.\relax }}{12}{table.caption.11}}
\newlabel{tab:uea_hyperparams_l2_results}{{5}{12}{Accuracy on the validation set for the hyperparmeter runs performed to determine the hyperparameters used in Section \ref {subsec:uea_classification}. The top column value represents the number of classes per shapelet for the run, with the lower values being the shapelet length proportion for that number of classes. The best run is given in bold. When multiple options achieved the highest score, the hyperparameters were chosen randomly from that top performing set.\relax }{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Accuracy on the validation set for the hyperparmeter runs performed to determine the hyperparameters used in Section \ref  {subsec:uea_missing_and_length}. The top column value represents the number of classes per shapelet for the run, with the lower values being the shapelet length proportion for that number of classes. The best hyperparameters are denoted in bold with a $^*$ and the worst length hyperperameter for the same number of shapelets per class denoted as bold followed by a $_*$. When multiple options achieved the highest score, the hyperparameters were chosen randomly from that top performing set.\relax }}{12}{table.caption.11}}
\citation{tensorflow}
\citation{pytorch}
\citation{jax}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Speech Commands}{13}{subsection.A.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}A further look at speech commands interpretability.}{13}{subsection.A.4}}
\newlabel{apx:further_speech_commands_interpretability}{{A.4}{13}{A further look at speech commands interpretability}{subsection.A.4}{}}
\newlabel{fig:old_speech_commands_axisplot}{{3a}{14}{Classical shapelet transform.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:old_speech_commands_axisplot}{{a}{14}{Classical shapelet transform.\relax }{figure.caption.12}{}}
\newlabel{fig:new_speech_commands_axisplot}{{3b}{14}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.12}{}}
\newlabel{sub@fig:new_speech_commands_axisplot}{{b}{14}{Generalised shapelet transform with $L^2$ discrepancy.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces All MFC coefficients for the shapelet (blue) and the training set minimizer (orange, dashed).\relax }}{14}{figure.caption.12}}
\newlabel{fig:speech_commands_axisplot}{{3}{14}{All MFC coefficients for the shapelet (blue) and the training set minimizer (orange, dashed).\relax }{figure.caption.12}{}}
